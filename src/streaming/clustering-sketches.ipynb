{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d6fe77-0d82-411a-8e9c-60b2abfd2387",
   "metadata": {},
   "source": [
    "# Reading and storing Pyspark dataframe to InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04f711d-6634-4f19-8036-a6b9b03a3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb_client in /opt/conda/lib/python3.10/site-packages (1.34.0)\n",
      "Requirement already satisfied: influxdb in /opt/conda/lib/python3.10/site-packages (5.3.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from influxdb_client) (1.26.11)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from influxdb_client) (65.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from influxdb_client) (2.8.2)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from influxdb_client) (2022.9.24)\n",
      "Requirement already satisfied: reactivex>=4.0.4 in /opt/conda/lib/python3.10/site-packages (from influxdb_client) (4.0.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from influxdb) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.17.0 in /opt/conda/lib/python3.10/site-packages (from influxdb) (2.28.1)\n",
      "Requirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from influxdb) (1.0.4)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from influxdb) (2022.6)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from reactivex>=4.0.4->influxdb_client) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.17.0->influxdb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.17.0->influxdb) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install influxdb_client influxdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71964600-eed6-4aa9-b24f-f47b48e4b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f9776b-5ca7-4bdd-b47c-3d14e76c2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cat(df): \n",
    "    df = df.copy() \n",
    "    df['LS 201'] = np.where(df['LS 201'] == 'Active', 1, 0)\n",
    "    df['LS 202'] = np.where(df['LS 202'] == 'Active', 1, 0)\n",
    "    df['LSL 203'] = np.where(df['LSL 203'] == 'Inactive', 0, 1)\n",
    "    df['LSLL 203'] = np.where(df['LSLL 203'] == 'Active', 1, 0)\n",
    "    df['LS 401'] = np.where(df['LS 401'] == 'Active', 1, 0)\n",
    "    df['LSH 601'] = np.where(df['LSH 601'] == 'Active', 1, 0)\n",
    "    df['LSH 602'] = np.where(df['LSH 602'] == 'Active', 1, 0)\n",
    "    df['LSH 603'] = np.where(df['LSH 603'] == 'Active', 1, 0)\n",
    "    df['LSL 601'] = np.where(df['LSL 601'] == 'Active', 1, 0)\n",
    "    df['LSL 602'] = np.where(df['LSL 602'] == 'Active', 1, 0)\n",
    "    df['LSL 603'] = np.where(df['LSL 603'] == 'Active', 1, 0)\n",
    "    return df\n",
    "attacks = [\"FIT 401\", \"UV401\", \"LIT 301\", \"P601 Status\", \"MV201\", \"P101 Status\", \"MV 501\", \"P301 Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892b8da7-ea6d-4330-ae52-ba5ca186957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+-----------+-----+-----------+------+-----------+\n",
      "|    FIT 401|UV401|   LIT 301|P601 Status|MV201|P101 Status|MV 501|P301 Status|\n",
      "+-----------+-----+----------+-----------+-----+-----------+------+-----------+\n",
      "|0.807990253|    2|   883.908|          1|    2|          2|     2|          2|\n",
      "|  0.8086305|    2|   883.908|          1|    2|          2|     2|          2|\n",
      "| 0.80927074|    2|883.387268|          1|    2|          2|     2|          2|\n",
      "| 0.80927074|    2|883.227051|          1|    2|          2|     2|          2|\n",
      "| 0.80927074|    2|883.227051|          1|    2|          2|     2|          2|\n",
      "+-----------+-----+----------+-----------+-----+-----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.appName(\"swat\").getOrCreate()\n",
    "\n",
    "df = pd.read_excel('SWaT_dataset_Jul 19 v2.xlsx', parse_dates = ['GMT +0'], index_col = 'GMT +0')[attacks]\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "df.index = df.index.tz_convert('Asia/Singapore') + pd.Timedelta(minutes=2)\n",
    "\n",
    "sdf = spark.createDataFrame(df) \n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32173edc-f833-4750-ad68-2ccf63791ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Online K-MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f776ad8e-b374-41f7-b7d7-500f065ef5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMean:\n",
    "    def __init__(self, k, d):\n",
    "        \"\"\"\n",
    "        Does an online k-means update on a single data point.\n",
    "        point - a 1 x d array\n",
    "        k - integer > 1 - number of clusters\n",
    "        cluster_means - a k x d array of the means of each cluster\n",
    "        cluster_counts - a 1 x k array of the number of points in each cluster\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.point = None\n",
    "        self.current_cluster = None\n",
    "        self.cluster_means = np.random.normal(size=[self.k, self.d], loc=0.5, scale=1)\n",
    "        self.cluster_counts = np.zeros((self.k,self.d))\n",
    "        \n",
    "    def add_point(self, point):\n",
    "        self.point = point\n",
    "    \n",
    "    def update(self):\n",
    "        cluster_distances = np.zeros(self.k)\n",
    "        for cluster in range(self.k):\n",
    "            cluster_distances[cluster] = sum(np.sqrt((self.point - self.cluster_means[cluster])**2))\n",
    "        self.current_cluster = np.argmin(cluster_distances)\n",
    "        self.cluster_counts[self.current_cluster] += 1\n",
    "        self.cluster_means[self.current_cluster] += 1.0/self.cluster_counts[self.current_cluster]*(self.point - self.cluster_means[self.current_cluster])\n",
    "    \n",
    "    def query(self):\n",
    "        return self.current_cluster\n",
    "    \n",
    "    def __call__(self, point):\n",
    "        self.add_point(point)\n",
    "        self.update()\n",
    "        return self.query()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b5491f-8f7b-43cc-8ec0-5dd24fe8181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "class BRICH:\n",
    "    def __init__(self, n_clusters=2, threshold=0.2):\n",
    "        \"\"\"\n",
    "        BIRCH clustering algorithm\n",
    "        point - 1 x d array\n",
    "        n_clusters - number of clusters\n",
    "        threshold - The radius of the subcluster \n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.threshold = threshold\n",
    "        self.point = None\n",
    "        self.current_cluster = None\n",
    "        self.brich = cluster.Birch(n_clusters=self.n_clusters,  threshold=self.threshold)\n",
    "        \n",
    "    def add_point(self, point):\n",
    "        self.point = point\n",
    "    \n",
    "    def update(self):\n",
    "        self.brich.partial_fit(np.array(self.point).reshape(1,-1))\n",
    "    \n",
    "    def query(self):\n",
    "        return self.brich.labels_[0].tolist()\n",
    "    \n",
    "    def __call__(self, point):\n",
    "        self.add_point(point)\n",
    "        self.update()\n",
    "        return self.query()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d62e6-b071-468e-b78a-ecb1bd4b2722",
   "metadata": {},
   "source": [
    "## Sending data to InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97938fe2-0732-4924-b01d-760ba5bc5245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_birch.py:752: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (2). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_birch.py:752: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (2). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "token = \"91XonVTK0cXpXmwjoF32jKOeMKIkf2LPK4Vm5lURZJKR2mNP9pftxRxvcBu487wUa0hPoggNTImKDFpzzxDQsQ==\"\n",
    "org = \"ost\"\n",
    "bucket = \"clustering\"\n",
    "url = \"http://influxdb:8086\"\n",
    "\n",
    "kmean = None\n",
    "brich = BRICH()\n",
    "with influxdb_client.InfluxDBClient(url=url, token=token, org=org) as client:\n",
    "    write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "    dataCollect = sdf.collect()\n",
    "    for row in dataCollect:\n",
    "        point = []\n",
    "        for col in sdf.columns:\n",
    "            p = influxdb_client.Point(\"point\").field(col, row[col])\n",
    "            write_api.write(bucket, org, p)\n",
    "            point.append(row[col])\n",
    "        if kmean is None:\n",
    "            kmean = KMean(2, len(point))\n",
    "        kmean_cluster = kmean(point)\n",
    "        brich_cluster = brich(point)\n",
    "        p = influxdb_client.Point(\"point\").field(\"kmean_cluster\", kmean_cluster)\n",
    "        write_api.write(bucket, org, p)\n",
    "        p = influxdb_client.Point(\"point\").field(\"brich_cluster\", brich_cluster)\n",
    "        write_api.write(bucket, org, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b368bcb-df36-43eb-b4ca-a239ae0d7223",
   "metadata": {},
   "source": [
    "## TO-DO CALL them within the conusmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3ce5a-26ca-45ec-a69d-99bb90118332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "70fb475e75512df3a1d0cbc0eecfa61af87fb4f720b7a8a7826b0ea889bf7e75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
